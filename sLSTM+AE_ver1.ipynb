{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfffe63b-9321-42b2-bed4-4dbdb323faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arch -q\n",
    "!git clone https://github.com/numenta/NAB.git -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8059bc2b-7172-4d58-a598-185722e3b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "class sRLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, forget_bias=1.0, dense=None,\n",
    "                 file_name='tweet', type='enc', component=1, partition=1, seed=None):\n",
    "        super(sRLSTMCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.forget_bias = forget_bias\n",
    "        self.file_name = file_name\n",
    "        self.type = type\n",
    "        self.component = component\n",
    "        self.partition = partition\n",
    "        self.step = 0  # Equivalent to TensorFlow's _step\n",
    "\n",
    "        # Initialize the main LSTM cell\n",
    "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
    "\n",
    "        # Initialize additional weights and biases\n",
    "        self.weight_h_2 = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.bias_h_2 = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        nn.init.trunc_normal_(self.weight_h_2, std=0.1)\n",
    "        nn.init.zeros_(self.bias_h_2)\n",
    "\n",
    "        # Activation function\n",
    "        self.activation = torch.tanh\n",
    "\n",
    "        # Initialize directories for saving masks\n",
    "        self._init_directories()\n",
    "\n",
    "        # Initialize mask generator\n",
    "        if seed is not None:\n",
    "            self.rng = np.random.RandomState(seed)\n",
    "        else:\n",
    "            self.rng = np.random.RandomState()\n",
    "\n",
    "    def _init_directories(self):\n",
    "        base_dir = f'./weight/{self.file_name}/{self.partition}/{self.component}/{self.type}'\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "        self.mask_dir = base_dir\n",
    "\n",
    "    def masked_weight(self, load=False):\n",
    "        mask_w1_path = os.path.join(self.mask_dir, f'W1_step_{self.step}.npy')\n",
    "        mask_w2_path = os.path.join(self.mask_dir, f'W2_step_{self.step}.npy')\n",
    "\n",
    "        if not load:\n",
    "            # Generate masks\n",
    "            masked_W1 = np.random.randint(2, size=self.hidden_size)\n",
    "            if masked_W1.sum() == 0:\n",
    "                masked_W2 = np.ones(self.hidden_size)\n",
    "            else:\n",
    "                masked_W2 = np.random.randint(2, size=self.hidden_size)\n",
    "                # Ensure at least one mask is active\n",
    "                if masked_W2.sum() == 0:\n",
    "                    masked_W2[0] = 1\n",
    "\n",
    "            # Save masks\n",
    "            np.save(mask_w1_path, masked_W1)\n",
    "            np.save(mask_w2_path, masked_W2)\n",
    "        else:\n",
    "            # Load masks\n",
    "            if os.path.exists(mask_w1_path) and os.path.exists(mask_w2_path):\n",
    "                masked_W1 = np.load(mask_w1_path)\n",
    "                masked_W2 = np.load(mask_w2_path)\n",
    "            else:\n",
    "                # If masks do not exist, generate them\n",
    "                masked_W1 = np.random.randint(2, size=self.hidden_size)\n",
    "                if masked_W1.sum() == 0:\n",
    "                    masked_W2 = np.ones(self.hidden_size)\n",
    "                else:\n",
    "                    masked_W2 = np.random.randint(2, size=self.hidden_size)\n",
    "                    if masked_W2.sum() == 0:\n",
    "                        masked_W2[0] = 1\n",
    "                np.save(mask_w1_path, masked_W1)\n",
    "                np.save(mask_w2_path, masked_W2)\n",
    "\n",
    "        # Convert masks to torch tensors\n",
    "        tf_mask_W1 = torch.tensor(masked_W1, dtype=torch.float32, device=self.weight_h_2.device)\n",
    "        tf_mask_W2 = torch.tensor(masked_W2, dtype=torch.float32, device=self.weight_h_2.device)\n",
    "        return tf_mask_W1, tf_mask_W2\n",
    "\n",
    "    def forward(self, input, state, load_mask=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: Tensor of shape (batch_size, input_size)\n",
    "            state: Tuple of (h, c), each of shape (batch_size, hidden_size)\n",
    "            load_mask: Boolean indicating whether to load masks from files\n",
    "        Returns:\n",
    "            h: New hidden state\n",
    "            new_state: Tuple of (new_h, new_c)\n",
    "        \"\"\"\n",
    "        h, c = state\n",
    "        self.step += 1\n",
    "\n",
    "        # Compute LSTM cell output\n",
    "        new_h_1, new_c = self.lstm(input, (h, c))\n",
    "\n",
    "        # Compute new_h_2\n",
    "        new_h_2 = torch.sigmoid(torch.matmul(h, self.weight_h_2) + self.bias_h_2)\n",
    "\n",
    "        # Get masks\n",
    "        mask_w1, mask_w2 = self.masked_weight(load=load_mask)\n",
    "\n",
    "        # Apply masks\n",
    "        new_h = new_h_1 * mask_w1 + new_h_2 * mask_w2\n",
    "\n",
    "        new_state = (new_h, new_c)\n",
    "        return new_h, new_state\n",
    "\n",
    "    def reset_step(self):\n",
    "        \"\"\"Reset the step counter.\"\"\"\n",
    "        self.step = 0\n",
    "\n",
    "    def save_masks(self):\n",
    "        \"\"\"Save current masks to files.\"\"\"\n",
    "        mask_w1, mask_w2 = self.masked_weight(load=False)\n",
    "        # Masks are already saved in masked_weight when load=False\n",
    "        # So no additional action is needed here\n",
    "        pass\n",
    "\n",
    "    def load_masks(self):\n",
    "        \"\"\"Load masks from files for the current step.\"\"\"\n",
    "        mask_w1, mask_w2 = self.masked_weight(load=True)\n",
    "        return mask_w1, mask_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1adff26-7463-4014-80d8-58a5d2f5569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, **kwargs):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # sRLSTMCell을 다층 구조로 사용하기 위해 ModuleList로 저장\n",
    "        self.cells = nn.ModuleList([\n",
    "            sRLSTMCell(\n",
    "                input_size if i == 0 else hidden_size,\n",
    "                hidden_size,\n",
    "                type='enc',\n",
    "                component=i+1,\n",
    "                **kwargs\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: Tensor of shape (seq_len, batch_size, input_size)\n",
    "        Returns:\n",
    "            outputs: List of final hidden states for each layer\n",
    "            states: List of final (h, c) tuples for each layer\n",
    "        \"\"\"\n",
    "        batch_size = inputs.size(1)\n",
    "        seq_len = inputs.size(0)\n",
    "\n",
    "        # 초기 은닉 상태와 셀 상태를 0으로 초기화\n",
    "        h = [torch.zeros(batch_size, self.hidden_size, device=inputs.device) for _ in range(self.num_layers)]\n",
    "        c = [torch.zeros(batch_size, self.hidden_size, device=inputs.device) for _ in range(self.num_layers)]\n",
    "        states = list(zip(h, c))\n",
    "\n",
    "        # 각 타임스텝에 대해 순환\n",
    "        for t in range(seq_len):\n",
    "            input_t = inputs[t]\n",
    "            for i, cell in enumerate(self.cells):\n",
    "                h_i, c_i = states[i]\n",
    "                h_i, (h_i, c_i) = cell(input_t, (h_i, c_i))\n",
    "                states[i] = (h_i, c_i)\n",
    "                input_t = h_i  # 다음 레이어의 입력으로 현재 레이어의 출력을 사용\n",
    "        outputs = [state[0] for state in states]  # 각 레이어의 마지막 은닉 상태\n",
    "        return outputs, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f76d86b-9ab2-491f-ac97-60e113ed2271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers=1, **kwargs):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # sRLSTMCell을 다층 구조로 사용하기 위해 ModuleList로 저장\n",
    "        self.cells = nn.ModuleList([\n",
    "            sRLSTMCell(\n",
    "                hidden_size,\n",
    "                hidden_size,\n",
    "                type='dec',\n",
    "                component=i+1,\n",
    "                **kwargs\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        # 최종 출력을 위한 선형 레이어\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, targets, encoder_states):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            targets: Tensor of shape (seq_len, batch_size, output_size)\n",
    "            encoder_states: List of (h, c) tuples from the encoder\n",
    "        Returns:\n",
    "            outputs: Reconstructed outputs of shape (seq_len, batch_size, output_size)\n",
    "        \"\"\"\n",
    "        batch_size = targets.size(1)\n",
    "        seq_len = targets.size(0)\n",
    "\n",
    "        # 인코더의 마지막 상태를 디코더의 초기 상태로 사용\n",
    "        h = [state[0].detach() for state in encoder_states]  # detach to prevent gradients flowing back to encoder\n",
    "        c = [state[1].detach() for state in encoder_states]\n",
    "        states = list(zip(h, c))\n",
    "\n",
    "        outputs = []\n",
    "        # 각 타임스텝에 대해 순환\n",
    "        for t in range(seq_len):\n",
    "            input_t = targets[t]\n",
    "            for i, cell in enumerate(self.cells):\n",
    "                h_i, c_i = states[i]\n",
    "                h_i, (h_i, c_i) = cell(input_t, (h_i, c_i))\n",
    "                states[i] = (h_i, c_i)\n",
    "                input_t = h_i  # 다음 레이어의 입력으로 현재 레이어의 출력을 사용\n",
    "            output_t = self.output_layer(input_t)\n",
    "            outputs.append(output_t)\n",
    "        outputs = torch.stack(outputs, dim=0)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d21ce56e-f54b-483a-a0b8-4e5f2d5a3e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, **kwargs):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size, num_layers, **kwargs)\n",
    "        self.decoder = Decoder(hidden_size, output_size, num_layers, **kwargs)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: Tensor of shape (seq_len, batch_size, input_size)\n",
    "            targets: Tensor of shape (seq_len, batch_size, output_size)\n",
    "        Returns:\n",
    "            outputs: Reconstructed outputs of shape (seq_len, batch_size, output_size)\n",
    "        \"\"\"\n",
    "        encoder_outputs, encoder_states = self.encoder(inputs)\n",
    "        outputs = self.decoder(targets, encoder_states)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20a982c-3bf5-4d57-8355-9ab47156dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleAutoEncoder(nn.Module):\n",
    "    def __init__(self, N, input_size, hidden_size, output_size, num_layers=1, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            N: Number of AutoEncoders in the ensemble\n",
    "            input_size: Size of the input features\n",
    "            hidden_size: Size of the hidden state in LSTM\n",
    "            output_size: Size of the output features\n",
    "            num_layers: Number of layers in each AutoEncoder\n",
    "            **kwargs: Additional keyword arguments for sRLSTMCell\n",
    "        \"\"\"\n",
    "        super(EnsembleAutoEncoder, self).__init__()\n",
    "        self.N = N\n",
    "        self.autoencoders = nn.ModuleList([\n",
    "            AutoEncoder(input_size, hidden_size, output_size, num_layers, **kwargs)\n",
    "            for _ in range(N)\n",
    "        ])\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: Tensor of shape (seq_len, batch_size, input_size)\n",
    "            targets: Tensor of shape (seq_len, batch_size, output_size)\n",
    "        Returns:\n",
    "            outputs: Averaged reconstructed outputs of shape (seq_len, batch_size, output_size)\n",
    "        \"\"\"\n",
    "        ensemble_outputs = []\n",
    "        for autoencoder in self.autoencoders:\n",
    "            output = autoencoder(inputs, targets)\n",
    "            ensemble_outputs.append(output)\n",
    "        # Stack and average the outputs\n",
    "        stacked_outputs = torch.stack(ensemble_outputs, dim=0)  # Shape: (N, seq_len, batch_size, output_size)\n",
    "        averaged_output = torch.mean(stacked_outputs, dim=0)    # Shape: (seq_len, batch_size, output_size)\n",
    "        return averaged_output\n",
    "\n",
    "    def reset_steps(self):\n",
    "        \"\"\"Reset step counters for all AutoEncoders in the ensemble.\"\"\"\n",
    "        for autoencoder in self.autoencoders:\n",
    "            for cell in autoencoder.encoder.cells:\n",
    "                cell.reset_step()\n",
    "            for cell in autoencoder.decoder.cells:\n",
    "                cell.reset_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69efcb8a-ca80-4261-966f-4fae4aa73247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # 딥러닝 재현성을 위해 추가 설정\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94df7b2e-5940-47f1-9c23-1f434f78165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 시드 설정\n",
    "random_seed = 777\n",
    "set_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db857fff-0a60-4655-8827-55d754c6f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-Do List\n",
    "# 1. Skip connection 구현\n",
    "# 2. Skip connection의 길이 L을 하이퍼파라미터로 설정할 수 있도록 수정\n",
    "# 3. h1, h2, mask 개념을 충실히 반영하여 기존 연결과 스킵 연결들을 랜덤하기 masking하도록 보장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f4d754-c2b4-4228-bd5d-be2960b90d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
