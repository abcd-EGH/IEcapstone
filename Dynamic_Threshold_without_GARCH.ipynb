{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b207e87-1567-40bd-b83e-9c26e148be9a",
   "metadata": {},
   "source": [
    "model library install\n",
    "```bash\n",
    "pip install git+https://github.com/abcd-EGH/srnn-ae.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9462a6d3-e8e5-47c6-ae2f-97fd63e3785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from srnn_ae import SLSTMAutoEncoder, set_random_seed\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score, cohen_kappa_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a13e76b7-b9b9-4a79-babc-71e353c973ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: srnn-ae\n",
      "Version: 0.1.1\n",
      "Summary: Sparsely connections RNN + AutoEncoder Model for Anomaly Detection in Time Series\n",
      "Home-page: https://github.com/abcd-EGH/srnn-ae\n",
      "Author: Jihwan Lee (abcd-EGH)\n",
      "Author-email: wlghks7790@gmail.com\n",
      "License: MIT\n",
      "Location: C:\\Users\\Jihwan\\anaconda3\\envs\\ml_env\\Lib\\site-packages\n",
      "Requires: numpy, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show srnn-ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "89844fc6-0801-4590-8cb0-48ae822d051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 및 전처리 함수\n",
    "def ReadNABDataset(_file_name, _normalize=True):\n",
    "    with open('./NAB/labels/combined_windows.json') as data_file:\n",
    "        json_label = json.load(data_file)\n",
    "    \n",
    "    # 데이터 디렉토리를 기준으로 상대 경로 계산\n",
    "    relative_path = os.path.relpath(_file_name, './NAB/data')\n",
    "    relative_path = relative_path.replace(os.sep, '/')  # 경로 구분자를 '/'로 통일\n",
    "    \n",
    "    print(f\"Processing file: {relative_path}\")  # 현재 처리 중인 파일명 출력\n",
    "    \n",
    "    list_windows = json_label.get(relative_path)\n",
    "    \n",
    "    if list_windows is None:\n",
    "        print(f\"Warning: No anomaly windows found for {relative_path}. All data labeled as normal.\")\n",
    "        abnormal = pd.read_csv(_file_name, header=0, index_col=0)\n",
    "        abnormal['label'] = 1\n",
    "    else:\n",
    "        abnormal = pd.read_csv(_file_name, header=0, index_col=0)\n",
    "        abnormal['label'] = 1\n",
    "        for window in list_windows:\n",
    "            start = window[0]\n",
    "            end = window[1]\n",
    "            abnormal.loc[start:end, 'label'] = -1\n",
    "\n",
    "    abnormal_data = abnormal['value'].values\n",
    "    abnormal_label = abnormal['label'].values\n",
    "\n",
    "    abnormal_data = np.expand_dims(abnormal_data, axis=1)\n",
    "    abnormal_label = np.expand_dims(abnormal_label, axis=1)\n",
    "\n",
    "    if _normalize:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        abnormal_data = scaler.fit_transform(abnormal_data)\n",
    "\n",
    "    # Normal = 1, Abnormal = -1\n",
    "    return abnormal_data, abnormal_label \n",
    "\n",
    "def prepare_data(file_name, window_size=288, partition=True, k_partition=10):\n",
    "    data, labels = ReadNABDataset(file_name)\n",
    "    \n",
    "    # 정상 데이터만 선택\n",
    "    normal_data = data[labels.flatten() == 1]\n",
    "    \n",
    "    # 윈도우 분할\n",
    "    windows = [normal_data[i:i+window_size] for i in range(len(normal_data) - window_size)]\n",
    "    \n",
    "    # 학습용과 검증용으로 분할 (80% 학습, 20% 검증)\n",
    "    train_data, val_data = train_test_split(windows, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    return np.array(train_data), np.array(val_data)\n",
    "\n",
    "def prepare_test_data(file_name, window_size=288):\n",
    "    data, labels = ReadNABDataset(file_name)\n",
    "    # 윈도우 분할\n",
    "    windows = [data[i:i+window_size] for i in range(len(data) - window_size)]\n",
    "    labels_window = [labels[i+window_size] for i in range(len(data) - window_size)]\n",
    "    windows = np.array(windows)  # Shape: (num_windows, window_size, 1) 예상\n",
    "    labels_window = np.array(labels_window)  # Shape: (num_windows, 1) 또는 (num_windows,)\n",
    "    \n",
    "    # 윈도우에 채널 차원이 없는 경우 추가\n",
    "    if len(windows.shape) == 2:\n",
    "        windows = windows[:, :, np.newaxis]  # Shape: (num_windows, window_size, 1)\n",
    "    \n",
    "    # 레이블 차원 맞추기 (필요시)\n",
    "    if len(labels_window.shape) == 1:\n",
    "        labels_window = labels_window[:, np.newaxis]  # Shape: (num_windows, 1)\n",
    "    \n",
    "    return windows, labels_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8e02cf8d-ccfc-4c58-9f6f-f5f79cb190d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter Setting Complete\n",
      "Processing file: artificialWithAnomaly/art_daily_jumpsup.csv\n",
      "Data Load Complete\n",
      "Model Initialization Complete\n"
     ]
    }
   ],
   "source": [
    "random_seed = 777\n",
    "\n",
    "set_random_seed(random_seed)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "N = 3  # 앙상블 모델 수, 10 or 40\n",
    "input_size = 1  # 단일 시계열\n",
    "hidden_size = 8\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "limit_skip_steps = 3 # L\n",
    "learning_rate = 1e-3\n",
    "l1_lambda = 0.005\n",
    "batch_size = 1024\n",
    "window_size = 288  # 시퀀스 길이 (1일 = 288 타임스텝 assuming 5분 간격)\n",
    "num_epochs = 20\n",
    "\n",
    "print(\"Hyperparameter Setting Complete\")\n",
    "\n",
    "# 데이터 준비\n",
    "file_name = './NAB/data/artificialWithAnomaly/art_daily_jumpsup.csv'\n",
    "train_data, val_data = prepare_data(file_name, window_size=window_size)\n",
    "\n",
    "# 텐서 변환 (unsqueeze 제거)\n",
    "train_tensor = torch.tensor(train_data, dtype=torch.float32)  # Shape: (num_samples, window_size, 1)\n",
    "val_tensor = torch.tensor(val_data, dtype=torch.float32)      # Shape: (num_samples, window_size, 1)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_dataset = TensorDataset(train_tensor, train_tensor)  # AutoEncoder이므로 입력과 출력이 동일\n",
    "val_dataset = TensorDataset(val_tensor, val_tensor)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "print(\"Data Load Complete\")\n",
    "\n",
    "# 디바이스 설정 (GPU가 사용 가능하면 GPU, 아니면 CPU)\n",
    "\n",
    "model = SLSTMAutoEncoder(\n",
    "    N=N, \n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    "    num_layers=num_layers,\n",
    "    limit_skip_steps=limit_skip_steps,\n",
    "    seed=random_seed\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Model Initialization Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1766a9d7-0f97-4930-9016-0bc40b88d867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 4.618236, Val Loss: 0.157708\n",
      "Epoch 2/20, Train Loss: 4.502486, Val Loss: 0.143149\n",
      "Epoch 3/20, Train Loss: 4.377482, Val Loss: 0.125683\n",
      "Epoch 4/20, Train Loss: 4.266998, Val Loss: 0.113209\n",
      "Epoch 5/20, Train Loss: 4.148487, Val Loss: 0.101525\n",
      "Epoch 6/20, Train Loss: 4.038438, Val Loss: 0.089954\n",
      "Epoch 7/20, Train Loss: 3.929043, Val Loss: 0.077927\n",
      "Epoch 8/20, Train Loss: 3.824141, Val Loss: 0.070361\n",
      "Epoch 9/20, Train Loss: 3.721596, Val Loss: 0.063656\n",
      "Epoch 10/20, Train Loss: 3.619125, Val Loss: 0.057940\n",
      "Epoch 11/20, Train Loss: 3.519444, Val Loss: 0.050411\n",
      "Epoch 12/20, Train Loss: 3.424964, Val Loss: 0.044521\n",
      "Epoch 13/20, Train Loss: 3.329379, Val Loss: 0.040660\n",
      "Epoch 14/20, Train Loss: 3.238148, Val Loss: 0.037098\n",
      "Epoch 15/20, Train Loss: 3.144913, Val Loss: 0.030508\n",
      "Epoch 16/20, Train Loss: 3.057587, Val Loss: 0.028398\n",
      "Epoch 17/20, Train Loss: 2.968727, Val Loss: 0.024458\n",
      "Epoch 18/20, Train Loss: 2.884354, Val Loss: 0.023338\n",
      "Epoch 19/20, Train Loss: 2.798319, Val Loss: 0.019387\n",
      "Epoch 20/20, Train Loss: 2.717830, Val Loss: 0.020263\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_inputs, batch_targets in train_loader:\n",
    "        batch_inputs = batch_inputs.permute(1, 0, 2).to(device)  # Shape: (window_size, batch_size, 1)\n",
    "        batch_targets = batch_targets.permute(1, 0, 2).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs, batch_targets)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        \n",
    "        # L1 정규화 추가\n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss += l1_lambda * l1_norm\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * batch_inputs.size(1)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # 검증\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_targets in val_loader:\n",
    "            batch_inputs = batch_inputs.permute(1, 0, 2).to(device)\n",
    "            batch_targets = batch_targets.permute(1, 0, 2).to(device)\n",
    "            \n",
    "            outputs = model(batch_inputs, batch_targets)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            val_loss += loss.item() * batch_inputs.size(1)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "45207465-fe9f-4a47-8511-909c1358d6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 가중치가 'ensemble_autoencoder_weights.pth'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 모델 가중치 저장\n",
    "torch.save(model.state_dict(), 'ensemble_autoencoder_weights_h1.pth')\n",
    "print(\"모델 가중치가 'ensemble_autoencoder_weights.pth'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4783caba-3f19-49bd-a834-3b14636c2ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 가중치가 로드되었습니다.\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('ensemble_autoencoder_weights_h1.pth', weights_only=True))\n",
    "print(\"모델 가중치가 로드되었습니다.\")\n",
    "\n",
    "# 디바이스 설정 (GPU가 사용 가능하면 GPU, 아니면 CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eacfcc2f-e8f3-40cf-9019-7db58cc4c253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test windows shape: (3744, 288, 1)\n",
      "Test labels window shape: (3744, 1)\n",
      "Batch 0 Complete\n",
      "Batch 1 Complete\n",
      "Batch 2 Complete\n",
      "Precision: 0.4976\n",
      "Recall: 0.2537\n",
      "F1-Score: 0.3361\n",
      "ROC AUC: 0.6857\n",
      "PR AUC: 0.2887\n",
      "Cohen Kappa: 0.2717\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 준비\n",
    "test_windows, test_labels_window = prepare_test_data(file_name, window_size=window_size)\n",
    "print(\"Test windows shape:\", test_windows.shape)  # 예: (6979, 288, 1)\n",
    "print(\"Test labels window shape:\", test_labels_window.shape)  # 예: (6979, 1)\n",
    "\n",
    "test_tensor = torch.tensor(test_windows, dtype=torch.float32)  # Shape: (num_windows, window_size, 1)\n",
    "test_labels_tensor = torch.tensor(test_labels_window, dtype=torch.float32)  # Shape: (num_windows, 1)\n",
    "\n",
    "# 테스트 데이터용 DataLoader 생성 (drop_last=True 추가)\n",
    "test_dataset = TensorDataset(test_tensor, test_labels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=0)\n",
    "\n",
    "# 재구성 오차 계산\n",
    "model.eval()\n",
    "reconstruction_errors = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (batch_inputs, _) in enumerate(test_loader):\n",
    "        # batch_inputs: (batch_size, window_size, 1)\n",
    "        batch_inputs = batch_inputs.permute(1, 0, 2).to(device)  # Shape: (window_size, batch_size, 1)\n",
    "        outputs = model(batch_inputs, batch_inputs)\n",
    "        loss = torch.mean((outputs - batch_inputs) ** 2, dim=(0, 2))  # Shape: (batch_size)\n",
    "        reconstruction_errors.extend(loss.cpu().numpy())\n",
    "        print(f'Batch {batch_idx} Complete')  # 디버깅용 출력\n",
    "\n",
    "# Z-스코어 계산\n",
    "errors = np.array(reconstruction_errors)\n",
    "z_scores = (errors - np.mean(errors)) / np.std(errors)\n",
    "\n",
    "# 임계값 설정 (예: Z-score > 3)\n",
    "threshold = 3\n",
    "y_pred = (z_scores > threshold).astype(int)\n",
    "\n",
    "# 실제 레이블 준비 (drop_last=True로 인해 일부 데이터는 무시됨)\n",
    "# len(errors) = batch_size * number_of_batches = 256 * 24 = 6144\n",
    "num_processed = len(errors)\n",
    "test_labels_window = test_labels_window[:num_processed]\n",
    "test_labels_binary = (test_labels_window.flatten() != 1).astype(int)  # 정상=0, 이상치=1\n",
    "\n",
    "# 성능 지표 계산\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    test_labels_binary, y_pred, average='binary', zero_division=0\n",
    ")\n",
    "roc_auc = roc_auc_score(test_labels_binary, errors[:num_processed])\n",
    "pr_auc = average_precision_score(test_labels_binary, errors[:num_processed])\n",
    "cks = cohen_kappa_score(test_labels_binary, y_pred)\n",
    "\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')\n",
    "print(f'ROC AUC: {roc_auc:.4f}')\n",
    "print(f'PR AUC: {pr_auc:.4f}')\n",
    "print(f'Cohen Kappa: {cks:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8afe61-980b-42ec-ac6f-87ea85c6fbb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
